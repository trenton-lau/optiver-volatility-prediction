
# Optiver Realized Volatility Prediction Analysis

**(!!! EDIT THIS README THOROUGHLY !!!)**

This project explores and compares different machine learning models (TabNet, SVM, XGBoost, LightGBM) for predicting short-term realized volatility, likely based on the Optiver Kaggle competition data.

## Project Goal

*(Describe the main objective: e.g., Predict volatility using book/trade data and compare model performance based on RMSPE.)*

## Data

*(**Crucial:** Explain where to get the ORIGINAL data)*

1.  **Source:** This project uses data from the **Optiver Realized Volatility Prediction competition on Kaggle**.
2.  **Link:** [https://www.kaggle.com/competitions/optiver-realized-volatility-prediction/data](https://www.kaggle.com/competitions/optiver-realized-volatility-prediction/data)
3.  **Required Files:** Download `train.csv`, `test.csv`, `book_train.parquet/`, `trade_train.parquet/`, `book_test.parquet/`, `trade_test.parquet/`.
4.  **Placement:** Place the downloaded files into the `data/original/` directory within this project structure (or update paths in the notebooks if placed elsewhere).
5.  **Processed Data:** The `.pkl` files (`new_train.pkl`, `new_test.pkl`) used by some notebooks are generated by running `notebooks/1_Data_Preprocessing.ipynb`. These large files are excluded by `.gitignore` and should *not* be committed.
6.  **Details:** Create a `data/README_DATA.md` file for any further specifics on data handling.

## Directory Structure


.
├── .gitignore # Files ignored by Git (includes data files)
├── README.md # This file - project overview (NEEDS UPDATE)
├── requirements.txt # Python dependencies (Generate using 'pip freeze > requirements.txt')
│
├── data/ # Data files (or instructions)
│ ├── README_DATA.md # Create this: Explain data sources/placement
│ ├── original/ # Original data (from Kaggle - content gitignored)
│ └── processed/ # Processed data (e.g., PKL files - content gitignored)
│
├── notebooks/ # Jupyter Notebooks for analysis and modeling
│ ├── 0_Data_Description.ipynb
│ ├── 1_Data_Preprocessing.ipynb # Generates processed data
│ ├── 2_Method_TabNet_SVM.ipynb
│ ├── 3_Method_XGBoost.ipynb
│ ├── 4_Method_LightGBM.ipynb
│ └── utils/ # Developmental/scratch notebooks
│ └── LightGBM_Trial.ipynb
│
├── scripts/ # (Optional) Reusable utility scripts - placeholder
│
└── reports/ # (Optional) Analysis summaries - placeholder

## Setup

*(Provide detailed, step-by-step setup instructions)*

1.  **Clone:** `git clone <your-repo-url>`
2.  **Navigate:** `cd <repo-name>`
3.  **Python Dependencies:**
    *   *(Ensure you have Python 3.x installed)*
    *   *(Consider creating a virtual environment)*
    *   Run `pip install -r requirements.txt` (**You need to create this file!**) Likely includes: `pandas`, `numpy`, `scikit-learn`, `pytorch-tabnet`, `torch`, `xgboost`, `lightgbm`, `category_encoders`, `pyarrow`, `fastparquet`, `seaborn`, `matplotlib`, `joblib`, `tqdm`.
4.  **Download Data:** Follow instructions in the **Data** section above and in `data/README_DATA.md`.
5.  **(Optional) Preprocess Data:** Run `notebooks/1_Data_Preprocessing.ipynb` to generate `new_train.pkl` and `new_test.pkl`.

## Usage

*(Explain how to run the key parts of the project)*
*   Example: Run notebooks in numerical order (0 to 4) in the `notebooks/` directory. Notebook 1 depends on original data and creates processed data needed by subsequent notebooks.

## Results

*(Summarize key findings, RMSPE scores for different models, best performing model, feature importances, etc.)*

## References

*   Optiver Realized Volatility Prediction Kaggle competition.
*   *(List any specific papers or libraries heavily relied upon)*
